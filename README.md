# DWH для данных о тенисных матчах

## План проекта
1. Развернуть Posgres в докер (создать docker-compose.yml )
2. Описать бизнес задачу и архитетктуру хранилища
3. Описать процесс обработки данных
4. Составить скрипты подключения в базе и источникам
3. Составить DDL скрипты для создания таблиц по слоям
4. Составить ETL скрипты для источников
5. Составить DML скрипты для распределения по слоям
6. Провести тестирование
7. Развернуть на DBaaS


### Бизнес - задача
Создать набор качественных данных для того, чтобы проанализировать влияние различных факторов на результат игры

### Архитектура
Подход Кимбалла, модель данных "Снежинка". Хранилище представляет собой набор слоев STG, DDS и CDM.

Так как нет возможности сразу получить данных о всех игроках, всех турнирах,
поэтому таблицы измерений будут наполняться по ходу заполнения таблицы фактов:
«Разрешение измерений по ходу загрузки фактов» / «Сначала факты, потом измерения». 

То есть сначала я буду наполнять таблицу фактов, из фактов пополняет таблицу измерений, а затем на основании новых записей в таблице измерений обогащают их информации из внешних источников адресно

Планируется загрузка данных о сыгранных матчах с двумя типами фактов:
- Факты о матче (то, что известно до матча: odd1, odd2, H2H-статистика).
- Факты о результативности (то, что известно после матча: все статистики по эйсам, подачам и т.д.).


#### STG 

stg.srv_wf_settings - сервисная таблица 
stg.mongo_matches - данные о фактах из источника
stg.mongo_players - данные об игроках из источника
stg.mongo_tours - данные об турнирах из источника

[STG](scripts/ddl/STG)

#### DDS

Таблицы измерений:
**dm_date** (Время), **dm_tour** (Турнир), **dm_player** (Игрок), **dm_player_rank** (Рейтинг игрока)

Таблицы фактов:
**fct_match_result** (Факты, известные ДО и СРАЗУ ПОСЛЕ матча), **fct_match_performance** (Детальная статистика ПОСЛЕ матча):

Индексы:
Индексы для внешних ключей таблиц фактов, а также для рейтинга (SCD2)

[DDS_dm_fct_tables_create.sql](scripts/ddl/3_DDS_dm_fct_tables_create.sql)


#### CDM




### Процесс обработки данных 

Данные планируется получать ежедневно обращаясь к источнику MongoDB на удалённом сервере, в дальнейшем планируется расширение списка источников

- Абстрагирую работу с источниками, напишу по классу-адаптеру для каждого, что позволит в будущем добавлять новые источники, не переписывая всю логику
- Использую Pydantic для базовой проверки схемы данных (типы колонок, обязательные поля)
- Создам таблицу с временем на несколько лет вперед
- Создам сервисные таблицы с информацией о добавлении новых данных в хранилище

- Обращусь к внешнему источнику и положу в STG данные о матчах: таблицы фактов (as is)

- Перед вставкой факта проверю существование всех связанных измерений и при их отсутствии — создам их.
- Преобразую данные и разложу их по таблицам фактов в слое DDS.

- Обращусь к внешнему источнику и положу в STG данные об измерениях: турнирах, игроках, их рейтингах (as is)
- Преобразую данные и разложу их по таблицам измерений в слое DDS.

- Соберу витрины для оценки данных в различных срезах. 
- Создам представления и экспериментов



### Составить скрипты подключения в базе и источникам






3. Составить DDL скрипты для создания таблиц по слоям
Генерируйте и наполняйте эту таблицу скриптом заранее на много лет вперед. Это стандартная практика.
4. Составить ETL скрипты для источников
5. Составить DML скрипты для распределения по слоям
6. Провести тестирование
7. Развернуть на DBaaS
